{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat 6-Minute Walk Tests Summary DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "sixmwt_root_path = \"/oak/stanford/groups/euan/projects/mhc/narayan/6mwt_merged\"\n",
    "sixmwt_summary_df = pd.read_parquet(os.path.join(sixmwt_root_path, \"private\", \"summary.parquet\"))\n",
    "\n",
    "\n",
    "pedo_df = pd.read_parquet(os.path.join(sixmwt_root_path, \"private\", \"pedometer.parquet\"))\n",
    "walk_hr_df = pd.read_parquet(os.path.join(sixmwt_root_path, \"private\", \"hr_walk.parquet\"))\n",
    "\n",
    "def _get_walk_hr_at_n_seconds(sub, n_seconds=tuple(range(30, 360+30, 30))):\n",
    "    try:\n",
    "        sub.startDate = pd.to_datetime(sub.startDate)\n",
    "        sub.endDate = pd.to_datetime(sub.endDate)\n",
    "    except TypeError:\n",
    "        return None\n",
    "\n",
    "    sub[\"delta\"] = sub.startDate.apply(lambda x: x - sub.startDate.iloc[0])\n",
    "    hr_at_n_seconds = []\n",
    "    for seconds in n_seconds:\n",
    "        try:\n",
    "            hr = sub[sub.delta <= pd.Timedelta(seconds=seconds)].iloc[-1].value\n",
    "        except IndexError:\n",
    "            hr = np.nan\n",
    "        hr_at_n_seconds.append(hr)\n",
    "    return hr_at_n_seconds\n",
    "\n",
    "def _get_distance_and_steps_at_n_seconds(sub, n_seconds=tuple(range(30, 360+30, 30))):\n",
    "    distances_at_n_seconds = []\n",
    "    number_of_steps_at_n_seconds = []\n",
    "    for seconds in n_seconds:\n",
    "        sub[\"delta\"] = sub.endDate - sub.startDate\n",
    "        try:\n",
    "            distance = sub[sub.delta <= pd.Timedelta(seconds=seconds)].iloc[-1].distance\n",
    "            steps = sub[sub.delta <= pd.Timedelta(seconds=seconds)].iloc[-1].numberOfSteps\n",
    "        except IndexError:\n",
    "            distance = np.nan\n",
    "            steps = np.nan\n",
    "        distances_at_n_seconds.append(distance)\n",
    "        number_of_steps_at_n_seconds.append(steps)\n",
    "\n",
    "    return distances_at_n_seconds, number_of_steps_at_n_seconds\n",
    "\n",
    "sixmwt_dems_pedos = []\n",
    "for _, row in sixmwt_summary_df.iterrows():\n",
    "    # Distance and Steps\n",
    "    sub = pedo_df[pedo_df.recordId == row[\"recordId\"]].copy()\n",
    "    if len(sub) == 0 or (sub.endDate - sub.startDate).dt.total_seconds().iloc[-1] < 350:\n",
    "        continue\n",
    "    \n",
    "    distances, steps = _get_distance_and_steps_at_n_seconds(sub)\n",
    "    row[\"distances_at_n_seconds\"] = distances\n",
    "    row[\"steps_at_n_seconds\"] = steps\n",
    "\n",
    "    # HR\n",
    "    sub = walk_hr_df[walk_hr_df.recordId == row[\"recordId\"]]\n",
    "    hr = None\n",
    "    if len(sub) >= 20:\n",
    "        hr = _get_walk_hr_at_n_seconds(sub)\n",
    "    row[\"hr_at_n_seconds\"] = hr\n",
    "    sixmwt_dems_pedos.append(row)\n",
    "\n",
    "df_full = pd.DataFrame(sixmwt_dems_pedos)\n",
    "df_full.to_parquet(\"/home/users/schuetzn/mhc_publication_0_data/full_summary_6mwt_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Multi-Walk DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DELTA_T = 7\n",
    "MAX_DELTA_T = 90\n",
    "\n",
    "df = df_sixmwt_dems_pedos.copy()\n",
    "df = df.sort_values(by=\"createdOn\")\n",
    "\n",
    "first_last_walk_df = df.groupby(\"healthCode\").first()\n",
    "first_last_walk_df = first_last_walk_df.join(df.groupby(\"healthCode\").last(), lsuffix=\"_t0\", rsuffix=\"_t1\")\n",
    "first_last_walk_df[\"healthCode\"] = first_last_walk_df.index\n",
    "first_last_walk_df.reset_index(drop=True, inplace=True)\n",
    "first_last_walk_df[\"t0_t1_delta\"] = first_last_walk_df[\"6mwt_startime_t1\"] - first_last_walk_df[\"6mwt_endtime_t0\"]\n",
    "\n",
    "first_last_walk_df[(first_last_walk_df.t0_t1_delta >= pd.Timedelta(days=MIN_DELTA_T)) & (first_last_walk_df.t0_t1_delta <= pd.Timedelta(days=MAX_DELTA_T))].to_parquet(\"~/multi_6mwts_dmin=%ddays_dmax=%ddays.parquet\" % (MIN_DELTA_T, MAX_DELTA_T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Filtered and Winsorized DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "def set_invalid_value_to_nan(df, target_column, lower_bound, upper_bound):\n",
    "    df = df.copy()\n",
    "    df.loc[(df[target_column] < lower_bound) | (df[target_column] > upper_bound), target_column] = np.nan\n",
    "    return df\n",
    "\n",
    "def filter_all_invalid_values(df, vals):\n",
    "    df = df.copy()\n",
    "    for val in vals:\n",
    "        df = set_invalid_value_to_nan(df, target_column=val[\"target_column\"], lower_bound=val[\"lower_bound\"], upper_bound=val[\"upper_bound\"])\n",
    "    return df\n",
    "\n",
    "def winsorize_all_values(df, vals, lower_percentile, upper_percentile):\n",
    "    df = df.copy()\n",
    "    for val in vals:\n",
    "        winsorize(df[val[\"target_column\"]], limits=(lower_percentile, upper_percentile), inclusive=(True, True))\n",
    "    return df\n",
    "\n",
    "vals = [\n",
    "        {\n",
    "            \"target_column\": \"HeightCentimeters\",\n",
    "            \"lower_bound\": 100,\n",
    "            \"upper_bound\": 220 \n",
    "        },\n",
    "        {\n",
    "            \"target_column\": \"WeightKilograms\",\n",
    "            \"lower_bound\": 40,\n",
    "            \"upper_bound\": 150 \n",
    "        },\n",
    "        {\n",
    "            \"target_column\": \"6mwt_total_distance\",\n",
    "            \"lower_bound\": 300,\n",
    "            \"upper_bound\": 1200 \n",
    "        },\n",
    "        {\n",
    "            \"target_column\": \"age\",\n",
    "            \"lower_bound\": 18,\n",
    "            \"upper_bound\": 90 \n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "df = df_sharable.copy()\n",
    "result_df = filter_all_invalid_values(df, vals)\n",
    "result_df = winsorize_all_values(result_df, vals, lower_percentile=0.05, upper_percentile=0.95)\n",
    "result_df.to_parquet(\"~/18k_6mwts_sharable_filtered_and_winsored.parquet\")\n",
    "\n",
    "df = df_full.copy()\n",
    "result_df = filter_all_invalid_values(df, vals)\n",
    "result_df = winsorize_all_values(result_df, vals, lower_percentile=0.05, upper_percentile=0.95)\n",
    "result_df.to_parquet(\"~/30k_6mwts_summary_filtered_and_winsored.parquet\")\n",
    "result_df.sort_values(by=\"createdOn\", inplace=True)\n",
    "result_df.groupby(\"healthCode\").first().to_parquet(\"~/8.9k_6mwts_first_walk_filtered_and_winsored.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymising Public Summary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "columns_anonymized = [\n",
    "    \"age\", \n",
    "    \"BiologicalSex\", \n",
    "    \"HeightCentimeters\", \n",
    "    \"WeightKilograms\",\n",
    "    '6mwt_total_distance',\n",
    "    '6mwt_total_steps',\n",
    "    'Stroke',\n",
    "    'Transient_Ischemic_Attack', \n",
    "    'Carotid_Artery_Blockage_Stenosis',\n",
    "    'Carotid_Artery_Surgery_or_Stent', \n",
    "    'Peripheral_Vascular_Disease',\n",
    "    'Abdominal_Aortic_Aneurysm', \n",
    "    'Pulmonary_Arterial_Hypertension',\n",
    "    'No_Vascular_Disease', \n",
    "    'Heart_Attack_Myocardial_Infarction',\n",
    "    'Heart_Bypass_Surgery', \n",
    "    'Coronary_Blockage_Stenosis',\n",
    "    'Coronary_Stent_Angioplasty', \n",
    "    'Angina_Heart_Chest_Pains',\n",
    "    'High_Coronary_Calcium_Score', \n",
    "    'Heart_Failure_or_CHF',\n",
    "    'Atrial_Fibrillation_Afib', \n",
    "    'Congenital_Heart_Defect',\n",
    "    'Pulmonary_Hypertension', \n",
    "    'No_Cardiovascular_Disease',\n",
    "    \"hr_at_n_seconds\",\n",
    "    \"walk_hr_mean\"\n",
    "]\n",
    "\n",
    "df = pd.read_parquet(\"/home/users/schuetzn/30k_6mwts_summary_filtered_and_winsored.parquet\")\n",
    "df_anonymized = df[columns_anonymized].copy()\n",
    "df_anonymized.age += np.random.normal(0, 3, len(df_anonymized))\n",
    "df_anonymized.HeightCentimeters += np.random.normal(0, 3, len(df_anonymized))\n",
    "df_anonymized.WeightKilograms += np.random.normal(0, 3, len(df_anonymized))\n",
    "df_anonymized.to_parquet(\"~/30k_6mwts_anonymized_filtered_and_winsored.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Sharable Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NONINFORMATIVE_COLUMNS = [\n",
    "    \"table_version\", \n",
    "    \"table_version__\",\n",
    "    \"zip_str\",\n",
    "    \"Age\",\n",
    "    \"emailVerified\",\n",
    "    \"sharingScope\",\n",
    "    \"withdrewOn\",\n",
    "    \"dataGroups\",\n",
    "    \"substudyMemberships\",\n",
    "    \"rawData\",\n",
    "    \"rawMetadata\",\n",
    "    \"birthdate\"\n",
    "]\n",
    "\n",
    "for c in NONINFORMATIVE_COLUMNS:\n",
    "    try:\n",
    "        df = df.drop(columns=c)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Name to Top 25 Zip Prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_prefix_to_state = {\n",
    "    '100': 'New York',\n",
    "    '112': 'New York',\n",
    "    '130': 'New York',\n",
    "    '200': 'Washington DC',\n",
    "    '222': 'Virginia',\n",
    "    '300': 'Georgia',\n",
    "    '326': 'Florida',\n",
    "    '331': 'Florida',\n",
    "    '334': 'Florida',\n",
    "    '501': 'Vermont',\n",
    "    '606': 'Illinois',\n",
    "    '774': 'Texas',\n",
    "    '900': 'California',\n",
    "    '939': 'California',\n",
    "    '940': 'California',\n",
    "    '941': 'California',\n",
    "    '943': 'California',\n",
    "    '945': 'California',\n",
    "    '950': 'California',\n",
    "    '951': 'California',\n",
    "    '956': 'California',\n",
    "    '972': 'Oregon',\n",
    "    '980': 'Washington',\n",
    "    '981': 'Washington',\n",
    "    '983': 'Washington'\n",
    "}\n",
    "\n",
    "def map_top25_zips(x):\n",
    "    if x in zip_prefix_to_state:\n",
    "        return zip_prefix_to_state[x]\n",
    "    if x == \"\":\n",
    "        return np.nan\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "df = df_full.copy()\n",
    "df[\"zip_str\"] = df.zip.astype(str).str[:3]\n",
    "df[\"zip_name\"] = df.zip_str.apply(map_top25_zips)\n",
    "df.drop(columns=\"zip_str\")\n",
    "df.to_parquet(\"~/30k_6mwts_summary.parquet\")\n",
    "\n",
    "df = df_sharable.copy()\n",
    "df[\"zip_str\"] = df.zip.astype(str).str[:3]\n",
    "df[\"zip_name\"] = df.zip.apply(map_top25_zips)\n",
    "df.drop(columns=\"zip_str\")\n",
    "\n",
    "df.to_parquet(\"~/18k_6mwts_sharable.parquet\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
